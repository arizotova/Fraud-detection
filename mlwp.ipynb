{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\"\n",
    "from model import SeismicNet\n",
    "from dense.densenet import FCDenseNet103\n",
    "from unet.models import NestedUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  2 14:05:36 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:61:00.0 Off |                  Off |\n",
      "| N/A   39C    P0    36W / 250W |   2031MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  Off  | 00000000:DB:00.0 Off |                  Off |\n",
      "| N/A   48C    P0    44W / 250W |   4381MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = SeismicNet(1, 1)\n",
    "#net = FCDenseNet103(1)\n",
    "net = NestedUNet(1, 1)\n",
    "net = net.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred,target):\n",
    "    numerator = 2 * torch.sum(pred * target)\n",
    "    denominator = torch.sum(pred + target)\n",
    "    return 1 - (numerator + 1) / (denominator + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(pred,target):\n",
    "    numerator = 2 * torch.sum(pred * target)\n",
    "    denominator = torch.sum(pred + target)\n",
    "    return (numerator + 1) / (denominator + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "img = np.zeros((1, 1, 256, 256))\n",
    "img = torch.from_numpy(img)\n",
    "out = net(img.float())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tversky_loss(true, logits, alpha, beta, eps=1e-7):\n",
    "    \"\"\"Computes the Tversky loss [1].\n",
    "    Args:\n",
    "        true: a tensor of shape [B, H, W] or [B, 1, H, W].\n",
    "        logits: a tensor of shape [B, C, H, W]. Corresponds to\n",
    "            the raw output or logits of the model.\n",
    "        alpha: controls the penalty for false positives.\n",
    "        beta: controls the penalty for false negatives.\n",
    "        eps: added to the denominator for numerical stability.\n",
    "    Returns:\n",
    "        tversky_loss: the Tversky loss.\n",
    "    Notes:\n",
    "        alpha = beta = 0.5 => dice coeff\n",
    "        alpha = beta = 1 => tanimoto coeff\n",
    "        alpha + beta = 1 => F beta coeff\n",
    "    References:\n",
    "        [1]: https://arxiv.org/abs/1706.05721\n",
    "    \"\"\"\n",
    "    num_classes = logits.shape[1]\n",
    "    if num_classes == 1:\n",
    "        true_1_hot = torch.eye(num_classes + 1)[true.squeeze(1)]\n",
    "        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
    "        true_1_hot_f = true_1_hot[:, 0:1, :, :]\n",
    "        true_1_hot_s = true_1_hot[:, 1:2, :, :]\n",
    "        true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n",
    "        pos_prob = torch.sigmoid(logits)\n",
    "        neg_prob = 1 - pos_prob\n",
    "        probas = torch.cat([pos_prob, neg_prob], dim=1)\n",
    "    else:\n",
    "        true_1_hot = torch.eye(num_classes)[true.squeeze(1)]\n",
    "        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "    true_1_hot = true_1_hot.type(logits.type())\n",
    "    dims = (0,) + tuple(range(2, true.ndimension()))\n",
    "    intersection = torch.sum(probas * true_1_hot, dims)\n",
    "    fps = torch.sum(probas * (1 - true_1_hot), dims)\n",
    "    fns = torch.sum((1 - probas) * true_1_hot, dims)\n",
    "    num = intersection\n",
    "    denom = intersection + (alpha * fps) + (beta * fns)\n",
    "    tversky_loss = (num / (denom + eps)).mean()\n",
    "    return (1 - tversky_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "A.ShiftScaleRotate(rotate_limit=20, )\n",
    "size = (256, 256)\n",
    "light = A.Compose([\n",
    "     #A.RandomSizedCrop((450, 450), 512, 512),     \n",
    "     A.ShiftScaleRotate(rotate_limit=20),\n",
    "     A.Blur(),\n",
    "     A.GaussNoise(),\n",
    "     A.Resize(size[0], size[1])\n",
    "],p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob, os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "class CovidDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=light):\n",
    "            self.data = pd.read_csv(csv_file)\n",
    "            self.root_dir = root_dir\n",
    "            self.transform = transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.data.iloc[idx, 1])\n",
    "        msk_name = os.path.join(self.root_dir,\n",
    "                                self.data.iloc[idx, 2])        \n",
    "        try:\n",
    "            image = cv2.imread(img_name, 0)\n",
    "            mask = cv2.imread(msk_name, 0) // 255\n",
    "        except:\n",
    "            print(img_name, msk_name)\n",
    "            pass\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=image, mask=mask)\n",
    "        else:\n",
    "            aug = dict()\n",
    "            aug['image'] = image\n",
    "            aug['mask'] = mask\n",
    "        \n",
    "        \n",
    "        return torch.from_numpy(aug['image'][np.newaxis]), torch.from_numpy(aug['mask'][np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15\n",
    "cov_dataset_train_bad = CovidDataset(csv_file='./data_train/train_bad.csv',\n",
    "                                     root_dir='./data_train/bad/')\n",
    "cov_dataset_val_bad = CovidDataset(csv_file='./data_train/val_bad.csv',\n",
    "                                     root_dir='./data_train/bad/', transform=None)\n",
    "cov_dataset_train_good = CovidDataset(csv_file='./data_train/train_good.csv',\n",
    "                                     root_dir='./data_train/good/')\n",
    "cov_dataset_val_good = CovidDataset(csv_file='./data_train/val_good.csv',\n",
    "                                     root_dir='./data_train/good/', transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train_bad = DataLoader(cov_dataset_train_bad, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=4)\n",
    "dataloader_val_bad = DataLoader(cov_dataset_val_bad, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=4)\n",
    "dataloader_train_good = DataLoader(cov_dataset_train_good, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=4)\n",
    "dataloader_val_good = DataLoader(cov_dataset_val_good, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /cv/volume/experiments/covid_nested_v5_new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "state_dict = torch.load('save/nested_v3/40.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in state_dict.items():\n",
    "    new_state_dict[k[7:]] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "#optimizer = optim.RMSprop(net.parameters(), lr=0.01, weight_decay=1e-8, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "name = 'covid_nested_v5_new_data'\n",
    "exp_path = '/cv/volume/experiments/{}'.format(name)\n",
    "writer = SummaryWriter(exp_path)\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)\n",
    "#criterion = nn.BCELoss()\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(7))\n",
    "net = DataParallel(net, device_ids=[0, 1]).train()\n",
    "net = net.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'covid_nested_v5_new_data'\n",
    "exp_path = '/cv/volume/experiments/{}'.format(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load('save/nested_v3/40.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(dataloader_val_bad):\n",
    "    break\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9d61b03836b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "n = 3\n",
    "plt.imshow(data[0].squeeze().cpu().detach().numpy()[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data[1].squeeze().cpu().detach().numpy()[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].squeeze().cpu().detach().numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader_val_bad)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i, data in enumerate(dataloader_val_bad):\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "global_step = 0\n",
    "s = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/envs/cv/lib/python3.6/site-packages/ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e5985e5d6240179cae08c7f3c88ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID_GOOD][BATCH 18/64] 0.13344215422157982 | 2.0592837333679234788\r"
     ]
    }
   ],
   "source": [
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "global_step = 0\n",
    "s = nn.Sigmoid()\n",
    "best_auc = 0\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "for epoch in tqdm(range(100)):\n",
    "    running_loss = 0.0\n",
    "    value = 0.0\n",
    "    \n",
    "    net.train()\n",
    "    i = 0\n",
    "    bad_iter = iter(dataloader_train_bad)\n",
    "    good_iter = iter(dataloader_train_good)\n",
    "    \n",
    "    \n",
    "    for step in range(len(dataloader_train_bad)):\n",
    "        try:\n",
    "            img_bad, mask_bad = next(bad_iter)\n",
    "            img_good, mask_good = next(good_iter)\n",
    "        except:\n",
    "            dataloader_train_good = DataLoader(cov_dataset_train_good, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=4)\n",
    "            dataloader_train_bad = DataLoader(cov_dataset_train_bad, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=4)\n",
    "            break\n",
    "        i += 1\n",
    "        \n",
    "        inputs = torch.cat([img_bad, img_good]).cuda().float()\n",
    "        masks = torch.cat([mask_bad, mask_good]).cuda().float()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, masks)\n",
    "        #loss = tversky_loss(masks.long(), outputs, 0.7, 0.3)\n",
    "        outputs = s(outputs)        \n",
    "        outputs[outputs > 0.6] = 1\n",
    "        outputs[outputs <= 0.6] = 0\n",
    "        \n",
    "        if global_step % 50 == 0:\n",
    "            x = torch.cat([img_bad[:2].detach().cpu().float(), mask_bad[:2].detach().cpu().float(), outputs[:2].detach().cpu().float()])\n",
    "            x = vutils.make_grid(x, normalize=True, scale_each=True)\n",
    "            writer.add_image('train/Image', x, global_step)  # Tensor\n",
    "            \n",
    "        dice = dice_loss(outputs, masks)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        writer.add_scalar('train/loss_CE', loss.item(), global_step)\n",
    "        writer.add_scalar('train/DICE', 1 - dice.item(), global_step)\n",
    "        global_step += 1\n",
    "        print(f\"[TRAIN][BATCH {i}/{len(dataloader_train_bad)}] {loss.item()} | {dice.item()}\\r\", end=\"\")   \n",
    "    \n",
    "    net.eval()\n",
    "    dice = 0\n",
    "    running_loss = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader_val_bad):\n",
    "            inputs = data[0].cuda().float()\n",
    "            masks = data[1].cuda().float()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, masks).detach()\n",
    "            #loss = tversky_loss(masks.long(), outputs, 0.7, 0.3).detach()\n",
    "            running_loss += loss.item()\n",
    "            outputs = s(outputs)\n",
    "            \n",
    "            \n",
    "            outputs[outputs > 0.75] = 1\n",
    "            outputs[outputs <= 0.75] = 0\n",
    "            \n",
    "            if len(outputs[outputs != 0]) > 0:\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "            y_true.append(1)\n",
    "            \n",
    "            if i == 0:\n",
    "                x = torch.cat([inputs[:2].detach().cpu().float(), masks[:2].detach().cpu().float(), outputs[:2].detach().cpu().float()])\n",
    "                x = vutils.make_grid(x, normalize=True, scale_each=True)\n",
    "                writer.add_image('val/image_bad', x, epoch)  # Tensor\n",
    "                \n",
    "            dice += dice_coef(outputs, masks)\n",
    "\n",
    "            print(f\"[VALID_BAD][BATCH {i}/{len(dataloader_val_bad)}] {running_loss / (i + 1)} | {dice / (i + 1)}\\r\", end=\"\")\n",
    "\n",
    "        for i, data in enumerate(dataloader_val_good):\n",
    "            inputs = data[0].cuda().float()\n",
    "            masks = data[1].cuda().float()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, masks).detach()\n",
    "            #loss = tversky_loss(masks.long(), outputs, 0.7, 0.3).detach()\n",
    "            running_loss += loss.item()\n",
    "            outputs = s(outputs)\n",
    "            \n",
    "            outputs[outputs > 0.75] = 1\n",
    "            outputs[outputs <= 0.75] = 0\n",
    "            \n",
    "            if len(outputs[outputs != 0]) > 0:\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "            y_true.append(0)\n",
    "            dice += dice_coef(outputs, masks)\n",
    "            \n",
    "            if i == 0:\n",
    "                x = torch.cat([inputs[:2].detach().cpu().float(), masks[:2].detach().cpu().float(), outputs[:2].detach().cpu().float()])\n",
    "                x = vutils.make_grid(x, normalize=True, scale_each=True)\n",
    "                writer.add_image('val/image_good', x, epoch)  # Tensor\n",
    "\n",
    "            print(f\"[VALID_GOOD][BATCH {i}/{len(dataloader_val_bad)}] {running_loss / (i + 1)} | {dice / (i + 1)}\\r\", end=\"\")\n",
    "        \n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        writer.add_scalar('val/loss_CE', running_loss / (len(dataloader_val_bad) + len(dataloader_val_good)), epoch)\n",
    "        writer.add_scalar('val/dice_loss', dice / (len(dataloader_val_bad) + len(dataloader_val_good)), epoch)\n",
    "        writer.add_scalar('val/accuracy', accuracy, epoch)\n",
    "        #scheduler.step(running_loss / (len(dataloader_val_bad) + len(dataloader_val_good)))\n",
    "        writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "        torch.save(net.module.state_dict(), 'save/{}/{}.pth.tar'.format(name, epoch))\n",
    "        if accuracy > best_auc:\n",
    "            torch.save(net.module.state_dict(), 'save/{}/best.pth.tar'.format(name))\n",
    "            best_auc = accuracy\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score([0, 0, 1], [1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.uint8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_bad.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.uint8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_bad.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No traceback has been produced, nothing to debug.\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No traceback has been produced, nothing to debug.\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), './save.pth.tar')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "net.load_state_dict(torch.load('./models/14.pth.tar'))\n",
    "net.eval()\n",
    "\n",
    "print('eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.cuda()\n",
    "#print('cuda')\n",
    "import torch\n",
    "\n",
    "a = [0, 0, 0]\n",
    "b = [1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/envs/cv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/miniconda/envs/cv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/miniconda/envs/cv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/miniconda/envs/cv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/miniconda/envs/cv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/miniconda/envs/cv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "print('eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for i, data in enumerate(dataloader_val_bad):\n",
    "    inputs = data[0].cuda().float()\n",
    "    masks = data[1].cuda().float()\n",
    "\n",
    "    outputs = net(inputs).detach()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 512, 512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = s(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "img = out[i].squeeze().detach().cpu().numpy()\n",
    "mask = masks[i].squeeze().detach().cpu().numpy()\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 1, 1, 1]),\n",
       " array([   512,    513,    514, ..., 262141, 262142, 262143]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(img.nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 1, 1, 1]),\n",
       " array([   0,    1,    2, ..., 4531, 4532, 4533]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(mask.nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.0556468e-05, 6.5137465e-06, 5.1085622e-06, ..., 4.4201506e-06,\n",
       "        5.1814486e-06, 2.6235362e-05],\n",
       "       [6.2028371e-06, 4.7883032e-07, 2.9595981e-07, ..., 2.5399638e-07,\n",
       "        4.1503981e-07, 5.5252667e-06],\n",
       "       [4.7944768e-06, 2.7802477e-07, 1.6657127e-07, ..., 1.2052726e-07,\n",
       "        2.3460022e-07, 4.3861000e-06],\n",
       "       ...,\n",
       "       [2.8513573e-06, 1.3606058e-07, 7.5703419e-08, ..., 6.3742348e-08,\n",
       "        1.4557675e-07, 3.2444168e-06],\n",
       "       [2.9685214e-06, 1.6397068e-07, 9.0943622e-08, ..., 9.7633034e-08,\n",
       "        2.0748362e-07, 3.7996001e-06],\n",
       "       [1.5303340e-05, 2.3283435e-06, 1.6350239e-06, ..., 1.7547600e-06,\n",
       "        2.7224407e-06, 1.9645160e-05]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
